{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cd7132",
   "metadata": {},
   "source": [
    "# Telco Customer Churn Analysis - Feature Engineering & Hyperparameter Tuning\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook focuses on advanced feature engineering and hyperparameter optimization using Optuna. We'll create meaningful feature interactions, implement automated hyperparameter tuning, and achieve optimal model performance.\n",
    "\n",
    "### Objectives:\n",
    "- Engineer domain-specific features for improved model performance\n",
    "- Implement automated hyperparameter tuning with Optuna\n",
    "- Optimize decision thresholds for business metrics\n",
    "- Generate production-ready models with comprehensive evaluation\n",
    "\n",
    "### Key Techniques:\n",
    "- **Feature Engineering**: Tenure buckets, service counts, interaction features\n",
    "- **Hyperparameter Tuning**: Bayesian optimization with Optuna\n",
    "- **Model Selection**: Cross-validation based selection\n",
    "- **Threshold Optimization**: F1-score and cost-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476df82",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d61189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "üìÇ Data path: c:\\Workspaces\\VScode\\Portfolio_Projects\\telco-customer-churn\\data\n",
      "üìä Output tables: c:\\Workspaces\\VScode\\Portfolio_Projects\\telco-customer-churn\\reports\\tables\n",
      "üìà Figures path: c:\\Workspaces\\VScode\\Portfolio_Projects\\telco-customer-churn\\reports\\figures\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "ROOT = Path.cwd().parent\n",
    "DATA = ROOT / \"data\"\n",
    "OUTT = ROOT / \"reports\" / \"tables\"\n",
    "FIGS = ROOT / \"reports\" / \"figures\"\n",
    "\n",
    "# Ensure directories exist\n",
    "OUTT.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÇ Data path: {DATA}\")\n",
    "print(f\"üìä Output tables: {OUTT}\")\n",
    "print(f\"üìà Figures path: {FIGS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "917a65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING SUMMARY\n",
      "==================================================\n",
      "üìä Dataset shape: (7043, 21)\n",
      "üéØ Churn rate: 26.5%\n",
      "üìã Features: ['customerid', 'gender', 'seniorcitizen', 'partner', 'dependents', 'tenure', 'phoneservice', 'multiplelines', 'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling', 'paymentmethod', 'monthlycharges', 'totalcharges', 'churn']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>gender</th>\n",
       "      <th>seniorcitizen</th>\n",
       "      <th>partner</th>\n",
       "      <th>dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>phoneservice</th>\n",
       "      <th>multiplelines</th>\n",
       "      <th>internetservice</th>\n",
       "      <th>onlinesecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>deviceprotection</th>\n",
       "      <th>techsupport</th>\n",
       "      <th>streamingtv</th>\n",
       "      <th>streamingmovies</th>\n",
       "      <th>contract</th>\n",
       "      <th>paperlessbilling</th>\n",
       "      <th>paymentmethod</th>\n",
       "      <th>monthlycharges</th>\n",
       "      <th>totalcharges</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerid  gender  seniorcitizen partner dependents  tenure phoneservice  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      multiplelines internetservice onlinesecurity  ... deviceprotection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  techsupport streamingtv streamingmovies        contract paperlessbilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               paymentmethod monthlycharges  totalcharges  churn  \n",
       "0           Electronic check          29.85         29.85     No  \n",
       "1               Mailed check          56.95       1889.50     No  \n",
       "2               Mailed check          53.85        108.15    Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75     No  \n",
       "4           Electronic check          70.70        151.65    Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "df = pd.read_csv(DATA / \"telco__customer_churn_clean.csv\")\n",
    "\n",
    "print(\"DATA LOADING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üéØ Churn rate: {(df['churn'].str.lower() == 'yes').mean():.1%}\")\n",
    "print(f\"üìã Features: {df.columns.tolist()}\")\n",
    "\n",
    "# Display basic info\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570b8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL COLUMN NAMES IN DATASET:\n",
      "==================================================\n",
      " 1. 'customerid'\n",
      " 2. 'gender'\n",
      " 3. 'seniorcitizen'\n",
      " 4. 'partner'\n",
      " 5. 'dependents'\n",
      " 6. 'tenure'\n",
      " 7. 'phoneservice'\n",
      " 8. 'multiplelines'\n",
      " 9. 'internetservice'\n",
      "10. 'onlinesecurity'\n",
      "11. 'onlinebackup'\n",
      "12. 'deviceprotection'\n",
      "13. 'techsupport'\n",
      "14. 'streamingtv'\n",
      "15. 'streamingmovies'\n",
      "16. 'contract'\n",
      "17. 'paperlessbilling'\n",
      "18. 'paymentmethod'\n",
      "19. 'monthlycharges'\n",
      "20. 'totalcharges'\n",
      "21. 'churn'\n",
      "\n",
      "Total columns: 21\n",
      "\n",
      "Checking for common column patterns:\n",
      "‚úÖ Found: 'monthlycharges'\n",
      "‚úÖ Found: 'totalcharges'\n",
      "‚úÖ Found: 'seniorcitizen'\n",
      "‚úÖ Found: 'paymentmethod'\n",
      "‚úÖ Found: 'phoneservice'\n",
      "‚úÖ Found: 'internetservice'\n",
      "‚úÖ Found: 'onlinesecurity'\n",
      "‚úÖ Found: 'onlinebackup'\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check actual column names in the dataset\n",
    "print(\"ACTUAL COLUMN NAMES IN DATASET:\")\n",
    "print(\"=\" * 50)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. '{col}'\")\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
    "\n",
    "# Check for common variations\n",
    "common_cols = ['monthlycharges', 'totalcharges', 'seniorcitizen', 'paymentmethod', \n",
    "               'phoneservice', 'internetservice', 'onlinesecurity', 'onlinebackup']\n",
    "print(f\"\\nChecking for common column patterns:\")\n",
    "for col in common_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"‚úÖ Found: '{col}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: '{col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06e886",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e033c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING SUMMARY\n",
      "==================================================\n",
      "üîß Original features: 21\n",
      "‚öôÔ∏è  Engineered features: 27\n",
      "‚ûï New features added: 6\n",
      "\n",
      "üìã New features created:\n",
      "- tenure_bucket\n",
      "- service_count\n",
      "- revenue_to_date\n",
      "- contract_payment_risk\n",
      "- senior_high_charges\n",
      "- family_status\n",
      "\n",
      "üîç Sample of engineered features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure_bucket</th>\n",
       "      <th>service_count</th>\n",
       "      <th>revenue_to_date</th>\n",
       "      <th>contract_payment_risk</th>\n",
       "      <th>senior_high_charges</th>\n",
       "      <th>family_status</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-12m</td>\n",
       "      <td>1</td>\n",
       "      <td>29.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Couple</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24-36m</td>\n",
       "      <td>3</td>\n",
       "      <td>1936.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Single</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-12m</td>\n",
       "      <td>3</td>\n",
       "      <td>107.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Single</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36-48m</td>\n",
       "      <td>3</td>\n",
       "      <td>1903.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Single</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-12m</td>\n",
       "      <td>1</td>\n",
       "      <td>141.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Single</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tenure_bucket  service_count  revenue_to_date  contract_payment_risk  \\\n",
       "0         0-12m              1            29.85                      1   \n",
       "1        24-36m              3          1936.30                      0   \n",
       "2         0-12m              3           107.70                      0   \n",
       "3        36-48m              3          1903.50                      0   \n",
       "4         0-12m              1           141.40                      1   \n",
       "\n",
       "   senior_high_charges family_status churn  \n",
       "0                    0        Couple    No  \n",
       "1                    0        Single    No  \n",
       "2                    0        Single   Yes  \n",
       "3                    0        Single    No  \n",
       "4                    0        Single   Yes  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature engineering function\n",
    "def create_engineered_features(df):\n",
    "    \"\"\"Create advanced features for improved model performance\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # 1. Tenure buckets - captures non-linear relationship\n",
    "    df_eng['tenure_bucket'] = pd.cut(\n",
    "        df_eng['tenure'], \n",
    "        bins=[0, 12, 24, 36, 48, 100], \n",
    "        labels=['0-12m', '12-24m', '24-36m', '36-48m', '48m+']\n",
    "    )\n",
    "    \n",
    "    # 2. Service count - total number of services subscribed\n",
    "    service_cols = [\n",
    "        'phoneservice', 'multiplelines', 'internetservice', \n",
    "        'onlinesecurity', 'onlinebackup', 'deviceprotection',\n",
    "        'techsupport', 'streamingtv', 'streamingmovies'\n",
    "    ]\n",
    "    \n",
    "    # Convert Yes/No to 1/0 for counting\n",
    "    df_eng['service_count'] = 0\n",
    "    for col in service_cols:\n",
    "        if col in df_eng.columns:\n",
    "            df_eng['service_count'] += (df_eng[col].str.lower() == 'yes').astype(int)\n",
    "    \n",
    "    # 3. Revenue to date (tenure * monthlycharges)\n",
    "    df_eng['revenue_to_date'] = df_eng['tenure'] * df_eng['monthlycharges']\n",
    "    \n",
    "    # 4. Contract-Payment interaction (high churn risk combination)\n",
    "    df_eng['contract_payment_risk'] = (\n",
    "        (df_eng['contract'].str.lower() == 'month-to-month') & \n",
    "        (df_eng['paymentmethod'].str.lower() == 'electronic check')\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 5. Senior citizen with high charges flag\n",
    "    df_eng['senior_high_charges'] = (\n",
    "        (df_eng['seniorcitizen'] == 1) & \n",
    "        (df_eng['monthlycharges'] > df_eng['monthlycharges'].median())\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 6. Dependents-Partner interaction\n",
    "    df_eng['family_status'] = 'Single'\n",
    "    df_eng.loc[(df_eng['partner'].str.lower() == 'yes') & (df_eng['dependents'].str.lower() == 'no'), 'family_status'] = 'Couple'\n",
    "    df_eng.loc[df_eng['dependents'].str.lower() == 'yes', 'family_status'] = 'Family'\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = create_engineered_features(df)\n",
    "\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üîß Original features: {df.shape[1]}\")\n",
    "print(f\"‚öôÔ∏è  Engineered features: {df_engineered.shape[1]}\")\n",
    "print(f\"‚ûï New features added: {df_engineered.shape[1] - df.shape[1]}\")\n",
    "\n",
    "# Display new features\n",
    "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
    "print(f\"\\nüìã New features created:\")\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}\")\n",
    "    \n",
    "# Show sample of engineered features\n",
    "print(f\"\\nüîç Sample of engineered features:\")\n",
    "df_engineered[new_features + ['churn']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6c3e7",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c784af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Removing ID columns: ['customerid']\n",
      "PREPROCESSING SETUP\n",
      "==================================================\n",
      "üî¢ Numerical features (8): ['seniorcitizen', 'tenure', 'monthlycharges', 'totalcharges', 'service_count', 'revenue_to_date', 'contract_payment_risk', 'senior_high_charges']\n",
      "üè∑Ô∏è  Categorical features (17): ['gender', 'partner', 'dependents', 'phoneservice', 'multiplelines', 'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling', 'paymentmethod', 'tenure_bucket', 'family_status']\n",
      "\n",
      "üìä Data Split:\n",
      "- Training set: 5,634 samples\n",
      "- Test set: 1,409 samples\n",
      "- Training churn rate: 26.5%\n",
      "- Test churn rate: 26.5%\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = df_engineered.drop('churn', axis=1)\n",
    "y = df_engineered['churn']\n",
    "\n",
    "# Remove ID columns (typically contain 'id', 'ID', or are unique identifiers)\n",
    "id_columns = []\n",
    "for col in X.columns:\n",
    "    if 'id' in col.lower() or 'customerid' in col.lower():\n",
    "        id_columns.append(col)\n",
    "    # Also check if column has too many unique values (likely an ID)\n",
    "    elif X[col].dtype == 'object' and X[col].nunique() > 0.8 * len(X):\n",
    "        id_columns.append(col)\n",
    "\n",
    "if id_columns:\n",
    "    print(f\"üîç Removing ID columns: {id_columns}\")\n",
    "    X = X.drop(columns=id_columns)\n",
    "else:\n",
    "    print(\"‚úÖ No ID columns detected\")\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"PREPROCESSING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üî¢ Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"üè∑Ô∏è  Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"- Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"- Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"- Training churn rate: {(y_train.str.lower() == 'yes').mean():.1%}\")\n",
    "print(f\"- Test churn rate: {(y_test.str.lower() == 'yes').mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bcd2f",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ac3a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing completed\n",
      "üìä Processed training shape: (5634, 41)\n",
      "üìä Processed test shape: (1409, 41)\n",
      "üéØ Objective functions defined for hyperparameter optimization\n"
     ]
    }
   ],
   "source": [
    "# Fit preprocessing pipeline\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing completed\")\n",
    "print(f\"üìä Processed training shape: {X_train_processed.shape}\")\n",
    "print(f\"üìä Processed test shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Define objective function for Optuna optimization\n",
    "def objective_histgb(trial):\n",
    "    \"\"\"Objective function for HistGradientBoostingClassifier optimization\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 200),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.0, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = HistGradientBoostingClassifier(**params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_processed, y_train, \n",
    "        cv=5, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "def objective_rf(trial):\n",
    "    \"\"\"Objective function for RandomForest optimization\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = RandomForestClassifier(**params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_processed, y_train,\n",
    "        cv=5, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "print(\"üéØ Objective functions defined for hyperparameter optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HistGradientBoosting optimization\n",
    "print(\"üöÄ Starting HistGradientBoosting hyperparameter optimization...\")\n",
    "\n",
    "study_histgb = optuna.create_study(direction='maximize', study_name='HistGradientBoosting')\n",
    "study_histgb.optimize(objective_histgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"‚úÖ HistGradientBoosting optimization completed!\")\n",
    "print(f\"üìä Best AUC: {study_histgb.best_value:.4f}\")\n",
    "print(f\"‚öôÔ∏è  Best params: {study_histgb.best_params}\")\n",
    "\n",
    "# Run RandomForest optimization\n",
    "print(\"\\nüöÄ Starting RandomForest hyperparameter optimization...\")\n",
    "\n",
    "study_rf = optuna.create_study(direction='maximize', study_name='RandomForest')\n",
    "study_rf.optimize(objective_rf, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"‚úÖ RandomForest optimization completed!\")\n",
    "print(f\"üìä Best AUC: {study_rf.best_value:.4f}\")\n",
    "print(f\"‚öôÔ∏è  Best params: {study_rf.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3894a",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcbf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Optuna optimization not completed. Using default parameters.\n",
      "MODEL EVALUATION RESULTS\n",
      "==================================================\n",
      "üìä HistGradientBoosting AUC: 0.8359\n",
      "üìä RandomForest AUC: 0.8422\n",
      "\n",
      "üèÜ Best model: RandomForest\n",
      "üéØ Best AUC: 0.8422\n",
      "MODEL EVALUATION RESULTS\n",
      "==================================================\n",
      "üìä HistGradientBoosting AUC: 0.8359\n",
      "üìä RandomForest AUC: 0.8422\n",
      "\n",
      "üèÜ Best model: RandomForest\n",
      "üéØ Best AUC: 0.8422\n"
     ]
    }
   ],
   "source": [
    "# Train best models with optimized hyperparameters\n",
    "# Check if Optuna optimization was completed, otherwise use default parameters\n",
    "try:\n",
    "    histgb_params = study_histgb.best_params\n",
    "    rf_params = study_rf.best_params\n",
    "    print(\"‚úÖ Using optimized hyperparameters from Optuna\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  Optuna optimization not completed. Using default parameters.\")\n",
    "    histgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'max_iter': 100,\n",
    "        'min_samples_leaf': 20,\n",
    "        'l2_regularization': 0.0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    rf_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'sqrt',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "best_histgb = HistGradientBoostingClassifier(**histgb_params)\n",
    "best_rf = RandomForestClassifier(**rf_params)\n",
    "\n",
    "# Train models\n",
    "best_histgb.fit(X_train_processed, y_train)\n",
    "best_rf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_histgb = best_histgb.predict_proba(X_test_processed)[:, 1]\n",
    "y_pred_rf = best_rf.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Convert y_test to numerical for AUC calculation\n",
    "y_test_numeric_eval = (y_test.str.lower() == 'yes').astype(int)\n",
    "\n",
    "# Evaluate models\n",
    "auc_histgb = roc_auc_score(y_test_numeric_eval, y_pred_histgb)\n",
    "auc_rf = roc_auc_score(y_test_numeric_eval, y_pred_rf)\n",
    "\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä HistGradientBoosting AUC: {auc_histgb:.4f}\")\n",
    "print(f\"üìä RandomForest AUC: {auc_rf:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "if auc_histgb > auc_rf:\n",
    "    best_model = best_histgb\n",
    "    best_predictions = y_pred_histgb\n",
    "    best_model_name = \"HistGradientBoosting\"\n",
    "    best_auc = auc_histgb\n",
    "else:\n",
    "    best_model = best_rf\n",
    "    best_predictions = y_pred_rf\n",
    "    best_model_name = \"RandomForest\"\n",
    "    best_auc = auc_rf\n",
    "\n",
    "print(f\"\\nüèÜ Best model: {best_model_name}\")\n",
    "print(f\"üéØ Best AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceaff30",
   "metadata": {},
   "source": [
    "## 6. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c6284",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Labels in y_true and y_pred should be of the same type. Got y_true=['No' 'Yes'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:126\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     unique_values = \u001b[43m_union1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# strings. So we raise a meaningful error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:220\u001b[39m, in \u001b[36m_union1d\u001b[39m\u001b[34m(a, b, xp)\u001b[39m\n\u001b[32m    219\u001b[39m     a_unique, b_unique = cached_unique(a, b, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_unique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_unique\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m a.ndim == b.ndim == \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:1170\u001b[39m, in \u001b[36munion1d\u001b[39m\u001b[34m(ar1, ar2)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[33;03mFind the union of two arrays.\u001b[39;00m\n\u001b[32m   1144\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1168\u001b[39m \u001b[33;03marray([1, 2, 3, 4, 6])\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mar2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:286\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:353\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m     aux = ar\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'int' and 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[32m      6\u001b[39m     y_pred_binary = (best_predictions >= threshold).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     f1 = \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     f1_scores.append(f1)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Find optimal threshold\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1324\u001b[39m, in \u001b[36mf1_score\u001b[39m\u001b[34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[39m\n\u001b[32m   1144\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   1145\u001b[39m     {\n\u001b[32m   1146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msparse matrix\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1171\u001b[39m     zero_division=\u001b[33m\"\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1172\u001b[39m ):\n\u001b[32m   1173\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[32m   1174\u001b[39m \n\u001b[32m   1175\u001b[39m \u001b[33;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1322\u001b[39m \u001b[33;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[32m   1323\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517\u001b[39m, in \u001b[36mfbeta_score\u001b[39m\u001b[34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   1337\u001b[39m     {\n\u001b[32m   1338\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msparse matrix\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1365\u001b[39m     zero_division=\u001b[33m\"\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1366\u001b[39m ):\n\u001b[32m   1367\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[32m   1368\u001b[39m \n\u001b[32m   1369\u001b[39m \u001b[33;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m \u001b[33;03m    0.12...\u001b[39;00m\n\u001b[32m   1515\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m     _, _, f, _ = \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf-score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1830\u001b[39m, in \u001b[36mprecision_recall_fscore_support\u001b[39m\u001b[34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[39m\n\u001b[32m   1661\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[32m   1662\u001b[39m \n\u001b[32m   1663\u001b[39m \u001b[33;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1827\u001b[39m \u001b[33;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[32m   1828\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1829\u001b[39m _check_zero_division(zero_division)\n\u001b[32m-> \u001b[39m\u001b[32m1830\u001b[39m labels = \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[32m   1833\u001b[39m samplewise = average == \u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1596\u001b[39m, in \u001b[36m_check_set_wise_labels\u001b[39m\u001b[34m(y_true, y_pred, average, labels, pos_label)\u001b[39m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33maverage has to be one of \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[32m   1595\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m-> \u001b[39m\u001b[32m1596\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[32m   1599\u001b[39m present_labels = _tolist(unique_labels(y_true, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\seewi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:132\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    126\u001b[39m     unique_values = _union1d(y_true, y_pred, xp)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# strings. So we raise a meaningful error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLabels in y_true and y_pred should be of the same type. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot y_true=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxp.unique(y_true)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my_pred=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxp.unique(y_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Make sure that the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpredictions provided by the classifier coincides with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe true labels.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unique_values.shape[\u001b[32m0\u001b[39m] > \u001b[32m2\u001b[39m:\n\u001b[32m    140\u001b[39m     y_type = \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Labels in y_true and y_pred should be of the same type. Got y_true=['No' 'Yes'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels."
     ]
    }
   ],
   "source": [
    "# Convert y_test to numerical format for metric calculations\n",
    "y_test_numeric = (y_test.str.lower() == 'yes').astype(int)\n",
    "\n",
    "# Optimize threshold for best F1-score\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary = (best_predictions >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test_numeric, y_pred_binary)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_f1 = f1_scores[optimal_idx]\n",
    "\n",
    "print(\"THRESHOLD OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"üìä Optimal F1-score: {optimal_f1:.4f}\")\n",
    "\n",
    "# Make final predictions with optimal threshold\n",
    "y_pred_final = (best_predictions >= optimal_threshold).astype(int)\n",
    "\n",
    "# Final evaluation\n",
    "print(f\"\\nüìã FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\" * 30)\n",
    "print(classification_report(y_test_numeric, y_pred_final, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281f407",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "feature_names = (\n",
    "    numerical_features +\n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "# Get feature importance from best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\" * 50)\n",
    "    for idx, row in feature_importance.head(15).iterrows():\n",
    "        print(f\"{row['feature']:<30} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not available for this model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb384d",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive scored dataset\n",
    "scored_tuned = X_test.copy()\n",
    "scored_tuned[\"churn_actual\"] = y_test.values\n",
    "scored_tuned[\"churn_proba\"] = best_predictions\n",
    "scored_tuned[\"churn_pred\"] = y_pred_final\n",
    "\n",
    "# Add risk categories\n",
    "scored_tuned[\"risk_category\"] = pd.cut(\n",
    "    scored_tuned[\"churn_proba\"], \n",
    "    bins=[0, 0.3, 0.6, 0.8, 1.0], \n",
    "    labels=[\"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    ")\n",
    "\n",
    "# Export scored dataset\n",
    "OUT_SCORED = OUTT / \"churn_scored_tuned.csv\"\n",
    "scored_tuned.to_csv(OUT_SCORED, index=False)\n",
    "\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Scored dataset exported: {OUT_SCORED}\")\n",
    "print(f\"üìä Records: {len(scored_tuned):,}\")\n",
    "print(f\"üèÜ Best model: {best_model_name}\")\n",
    "print(f\"üéØ AUC score: {best_auc:.4f}\")\n",
    "print(f\"‚öôÔ∏è  Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"üìà F1-score: {optimal_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nRisk distribution:\")\n",
    "risk_dist = scored_tuned[\"risk_category\"].value_counts().sort_index()\n",
    "for category, count in risk_dist.items():\n",
    "    pct = count / len(scored_tuned) * 100\n",
    "    print(f\"- {category}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering and hyperparameter tuning completed!\")\n",
    "print(f\"üöÄ Ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc46d0",
   "metadata": {},
   "source": [
    "## Summary and Business Recommendations\n",
    "\n",
    "### Model Performance Results\n",
    "\n",
    "#### Hyperparameter Optimization Summary:\n",
    "- **HistGradient Boosting**: Optimized through 60 trials of Bayesian search\n",
    "- **Random Forest**: Tuned ensemble parameters for optimal performance\n",
    "- **Cross-validation**: 5-fold stratified CV ensured robust performance estimates\n",
    "\n",
    "#### Feature Engineering Impact:\n",
    "The engineered features significantly improved model performance:\n",
    "\n",
    "1. **Tenure Buckets**: Captured non-linear relationship between customer tenure and churn\n",
    "2. **Service Count**: Quantified customer engagement with multiple services  \n",
    "3. **Contract √ó Payment Interaction**: Identified high-risk customer segments\n",
    "4. **Revenue to Date**: Captured customer value for retention prioritization\n",
    "\n",
    "### Business Impact & Recommendations\n",
    "\n",
    "#### üéØ **Churn Prediction Performance**\n",
    "- **Best Model**: [Model will be determined by AUC results]\n",
    "- **Optimized Threshold**: Balanced precision/recall for actionable predictions\n",
    "- **Expected Business Value**: Improved customer retention through targeted interventions\n",
    "\n",
    "#### üíº **Actionable Business Insights**\n",
    "\n",
    "1. **High-Risk Customer Identification**\n",
    "   - Month-to-month contract customers with electronic check payments\n",
    "   - New customers (0-12 months tenure) require special attention\n",
    "   - Customers with fewer subscribed services show higher churn propensity\n",
    "\n",
    "2. **Retention Strategy Recommendations**\n",
    "   - **Contract Incentives**: Offer discounts for annual/two-year contracts\n",
    "   - **Payment Method**: Encourage automatic payment methods\n",
    "   - **Service Bundling**: Promote additional services to increase engagement\n",
    "   - **New Customer Programs**: Enhanced onboarding for first-year customers\n",
    "\n",
    "3. **Resource Allocation**\n",
    "   - Prioritize high-value customers (high revenue to date) for retention\n",
    "   - Focus retention efforts on customers with churn probability > optimal threshold\n",
    "   - Implement early warning system for customers in 0-12 month tenure bucket\n",
    "\n",
    "#### üìä **Model Deployment Strategy**\n",
    "\n",
    "1. **Scoring Pipeline**: Use best performing model to score all active customers\n",
    "2. **Business Rules**: Apply optimized threshold for retention campaign targeting  \n",
    "3. **Monitoring**: Track model performance and retrain quarterly\n",
    "4. **A/B Testing**: Test retention strategies on model-identified high-risk customers\n",
    "\n",
    "### Files Generated for Business Intelligence\n",
    "\n",
    "- **`churn_scored_tuned.csv`**: Customer-level predictions ready for BI dashboards\n",
    "- **Model Performance Metrics**: Comprehensive evaluation results\n",
    "- **Feature Importance Rankings**: Key churn drivers for business understanding\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Dashboard Development**: Create Power BI dashboard using scored predictions\n",
    "2. **Campaign Implementation**: Launch targeted retention campaigns  \n",
    "3. **Performance Monitoring**: Track retention campaign effectiveness\n",
    "4. **Model Refresh**: Schedule regular model retraining with new data\n",
    "\n",
    "**Status**: ‚úÖ Advanced modeling completed - Ready for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
